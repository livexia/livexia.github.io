<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[Kali学习笔记一]]></title>
      <url>%2F2017%2F04%2F29%2Fkali_1%2F</url>
      <content type="text"><![CDATA[笔记内容：WiFi Wireless Security 在虚拟机中安装kali Linux，完成启动系统后的第一次更新。123apt updateapt upgradeapt dist-upgrade 需要购买合适的无线网卡来进行测试，笔记本自带网卡可能无法运行在所需要的模式。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[The sixth week|第六周]]></title>
      <url>%2F2017%2F04%2F23%2Fpost_7%2F</url>
      <content type="text"><![CDATA[代码地址：livexia/requests_bs4 完成对某一歌手(热门50单曲)／专辑／歌单的所有歌曲的爬取。使用方式：python netease_music.py &lt; playlist/artist/album &gt; &lt; 对应id &gt; 安装mysql，利用docker容器。 python这边利用pymysql，利用sql语句插入，表结构目前为三张表song、author、album由于编码问题，导致数据库的汉字为乱码。 解决办法：在创建数据库时添加 character set utf8 例如： create database NeteaseMusic character set utf8;python代码中对需要输入的数据，通过encode函数，例如：info[key][0].encode(‘utf-8’)数据结构： song.sql: song_id title author_id album_id author.sql: author_id author_name album.sql: album_id album_name pic_url_link 新增存入mysql可选 分析网易云音乐404页面，当用户输入错误时，可以输出错误信息。 网易云音乐的404，不同于一般网站的404，网易云音乐类似于当输入id为不存在的值得时候，返回内部404元素。可以利用这一点来写输入错误的判断。 利用bs4解析，寻找class=‘n-for404’，若存在即为404，用户输入错误的id。程序退出 本周由于在复习概率论，导致这个异常部分没有完成，检测404还是存在部分问题。 TODO：完成异常错误部分，增加函数爬取歌手所有专辑，然后得到专辑链接，再对歌手所有进行爬起，可以得到歌手所有歌曲信息。同时在这个过程中增加随机UA本周完]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[The fifth week|第五周]]></title>
      <url>%2F2017%2F04%2F16%2Fpost_6%2F</url>
      <content type="text"><![CDATA[在上周尝试爬取网易云音乐，得到了网易云音乐的api，在听取老师建议下，我决定先暂时不利用网易云音乐的api。 尝试不使用api的方法来抓取内容，大体上现在有两种方法均可以达到目的， 一是scrapy+splash/selenium… 二是利用requests+bs4 第一种方法由于需要部署环境且在爬取过程中需要花费大量的时间，并且存在数据丢失问题 我在这一周暂时只考虑利用requests+bs4来实现对单一歌单的爬取，即第二种方法。 通过对网站的分析，可以发现歌单内歌曲信息都在textarea标签下，并且为json格式。 于是利用bs4获取标签textarea下的所有文本，再import json解析json 获取所需要的信息，并利用歌曲／歌手／专辑id来获取歌曲／歌手／专辑的链接，来进行进一步利用。 代码地址：livexia/requests_bs4 使用方式：python netease_music.py playlist &lt;歌单：id&gt; 下周TODO：爬取某一歌手(热门50单曲)／专辑的所有歌曲，增加异常的判断，增加随机UA，将数据保存在mysql中。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[有意思的故事]]></title>
      <url>%2F2017%2F04%2F10%2Famusing%2F</url>
      <content type="text"><![CDATA[2017-04-10 20:19:22+08:00从前有个小孩很调皮，他们村是一个过路村，每天都有很多陌生人从他们村口经过，有一天这个熊孩子站在村口的树上对着路过的人撒尿，结果碰到一个大叔被尿了一身，大叔让小孩下来，非但没有责怪他，还夸他尿得远，尿得多，然后笑呵呵的走了。回家后大叔的妻子听说后就问他为什么不教训那个孩子一顿，大叔笑着说：我又不是他爹，干嘛帮他教育孩子啊？等别人帮他吧！故事的结局就是不到一年，就听说那个孩子被当地一个恶霸打死了……]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[歌曲推荐：Seasons In The Sun]]></title>
      <url>%2F2017%2F04%2F10%2Fmusic_4%2F</url>
      <content type="text"><![CDATA[演唱：Westlife、Nirvana 歌手：Black Box Recorder 专辑：The Worst Of Black Box Recorder 语种：欧美 曲风：慢摇 注：由于版权问题Westlife无法外链播放 Seasons In The Sun注：歌词来自Black Box Recorder版本Goodbye to you, my trusted friend.再见了 我的挚友We’ve known each other since we were nine or ten.我们从十岁左右就认识了Together we’ve climbed hills and trees,我们一起爬山爬树learned of love and ABC’s,一起学习爱与字母skinned our hearts and skinned our knees.共同经历成长的伤痛Goodbye, my friend.再见了 我的朋友It’s hard to die离开你真的很难when all the birds are singing in the sky.当所有的鸟儿都在天空尽情歌唱Now that spring is in the air所有的花儿都盛开烂漫pretty boys are everywhere.美丽的女孩儿随处可见Think of me and I’ll be there.想想我 我就要去那里了Goodbye, Papa, please pray for me.再见了 爸爸 请为我祈祷吧I was the black sheep of the family.我像是家中的害群之马一般You tried to teach me right from wrong.你试着教我分清是非Too much wine and too much song.我却沉迷于酒乐Wonder how I got along.真不知我是如何过来的Goodbye, papa.再见了 爸爸It’s hard to die when all the birds are singing in the sky.当所有的鸟儿都在天空尽情歌唱 离开你真的很难Now that the spring is in the air,闻到了初春的气息Little children everywhere.到处都会是孩子的身影When you see them, I’ll be there.当你看着他们 我就会在那里 Goodbye, Michelle, my precious one.再见了 Michelle 我的小宝贝You gave me love and helped me find the sun.你给予了我爱和阳光And every time that I was down,每当我沮丧的时候you would always come around,你总是陪在我身边and get my feet back on the ground.让我重拾信心Goodbye, Michelle.再见了 MichelleIt’s hard to die when all the birds are singing in the sky当所有的鸟儿都在天空尽情歌唱 离开你真的很难Now that spring is in the air,闻到了初春的气息with the flowers everywhere,所有的花儿都盛开烂漫I wish that we could both be there.我希望我们能一起享受春光We had joy, we had fun,我们曾有过欢声笑语we had seasons in the sun,我们曾一起度过阳光四季but the hills that we climbed were just seasons out of time.但我们一起爬山的美好时光 都已一去不返We had joy, we had fun,we had seasons in the sun,我们曾有过欢声笑语 我们曾一起度过阳光四季but the stars we could reach were just starfish on the beach.但我们所接触的星辰 根本就微不足道 链接网易云／Black Box Recorder网易云／Nirvana网易云／Westlife]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[The fourth week|第四周]]></title>
      <url>%2F2017%2F04%2F09%2Fpost_5%2F</url>
      <content type="text"><![CDATA[TODO:加入其他反反爬虫功能，尝试编写针对网易云的爬虫 由于豆瓣top250没有什么反爬虫，于是我决定对网易云音乐下手，本周先对给定歌单进行获取，得到歌单的基本信息。 新建scrapy项目netease_music, 增加spider，name = getlist 尝试老方法对页面进行爬取书发现返回为js代码段。 由于网易云将所有实质内容存放在iframe标签呢，利用request方法无法取得页面内容。 在网上也看到有人利用selenium先得到iframe标签内内的html代码，再用xpath，对其进行获取。 我略微搜索发现selenium本身测试平台，拿来跑爬虫效率太低，便决定寻求其他方法。 之后在知乎上发现有网易云的api，github：metowolf/NeteaseCloudMusicApi，metowolf/Meting 采取使用网易云音乐的api方式来获取歌单信息。利用api可以简单获取到含有歌单所有信息的json文件，不需要scrapy进行爬取。 利用api也可以抓取评论，找到API简直省力几万倍 总结：本周大部分时间都花在如何解析iframe中，先有抓取方案过于臃肿浪费，找到网易云音乐api省事不少，具体实现本周尚未写好，由于api直接可以导出json，因此这周大部分针对抓取单一歌单的信息的努力直接实现。对于抓取到的.json文件，也可以通过python解析，获取需要的数据。 TODO：解析通过api获得的json文件，留取需要的数据，存入数据库中。模拟登陆网易云，爬取个人的所有歌单，利用网易云音乐api，获得json文件，将其存储在mongdb中。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[个人随想]]></title>
      <url>%2F2017%2F04%2F08%2Fcaprice%2F</url>
      <content type="text"><![CDATA[2017-04-18 21:02:14+08:00终究意难平 2017-04-17 21:12:10+08:00（出自网易云音乐评论，歌曲：凉城）我又不乱来_T: 我有一瓶09年9月26日她随手递给我的统一冰红茶。至今未拆。 2017-04-11 22:27:40+08:00（歌曲小智的歌词）你爸爸对你说过 做人要仗义善良 但唯有女人当仁不让你妈妈对你说过 做人要心胸开阔 但唯有傻逼不能惯着 2017-04-10 19:57:59+08:00（出自网易云音乐评论）綠色的橘汁: 不怕流氓有文化 就怕流氓有吉他 2017-04-08 23:58:38+08:00突然看到小说里面，陆总是被打成6，之前以为是巧用，今晚却突然想到有可能是，由于口音问题陆6不分。。。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[推荐过的歌曲]]></title>
      <url>%2F2017%2F04%2F05%2Fmusic_list%2F</url>
      <content type="text"><![CDATA[最近更新：2017-04-10 11:46:19+08:00 注：部分歌曲由于版权问题，无法在线播放 链接网易云]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[歌曲推荐：我的一个道姑朋友（Cover：Lon）]]></title>
      <url>%2F2017%2F04%2F05%2Fmusic_3%2F</url>
      <content type="text"><![CDATA[演唱：以冬 专辑：我的一个道姑朋友 语种：华语 曲风：流行、古风 我的一个道姑朋友（Cover：Lon）作曲 : 《一番星》作词 : 陆菱纱原唱：LON后期：圣雨轻纱 而你撑伞拥我入怀中，一字一句誓言多慎重。你眼中有柔情千种，如脉脉春风，冰雪也消融。那年长街春意正浓，策马同游，烟雨如梦。檐下躲雨，望进一双，深邃眼瞳，宛如华山夹着细雪的微风。 雨丝微凉，风吹过暗香朦胧。一时心头悸动，似你温柔剑锋，过处翩若惊鸿。 是否情字写来都空洞，一笔一画斟酌着奉送，甘愿卑微换个笑容，或沦为平庸。 而你撑伞拥我入怀中，一字一句誓言多慎重。你眼中有柔情千种，如脉脉春风，冰雪也消融。 后来谁家喜宴重逢，佳人在侧，烛影摇红。 灯火缱绻，映照一双，如画颜容，宛如豆蔻枝头温柔的旧梦。 对面不识，恍然间思绪翻涌。望你白衣如旧，神色几分冰冻，谁知我心惶恐？ 也许我应该趁醉装疯，借你怀抱留一抹唇红。再将旧事轻歌慢诵，任旁人惊动。 可我只能假笑扮从容，侧耳听那些情深意重。不去看你熟悉脸孔，只默默饮酒，多无动于衷。 山门外，雪拂过白衣，又在指尖消融；负长剑，试问江湖偌大，该何去何从？今生至此，像个笑话一样，自己都嘲讽，一厢情愿，有始无终。 若你早与他人两心同，何苦惹我错付了情衷。难道看我失魂落魄，你竟然心动？ 所幸经年漂浮红尘中，这颗心已是千疮百孔。怎惧你以薄情为刃，添一道裂缝？ 又不会痛。 不如将往事埋在风中，以长剑为碑，以霜雪为冢。此生若是错在相逢，求一个善终。 孤身打马南屏旧桥边过，恰逢山雨来时雾蒙蒙。想起那年伞下轻拥，就像躺在桥索之上，做了一场梦，梦醒后跌落，粉身碎骨，无影亦无踪。 链接网易云]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[歌曲推荐：遥远的歌]]></title>
      <url>%2F2017%2F04%2F04%2Fmusic_2%2F</url>
      <content type="text"><![CDATA[演唱：Cannie 专辑：Bittersweet 语种：华语 曲风：流行 遥远的歌词/曲/编曲/演唱：Cannie Y. 写了一首遥远的歌送给遥远的你 你的笑声我的歌声编织在一起 这是我对旧时光最温暖的回忆 哭着笑着痛着疯着跟过去别离 旧旧的琴弦在吟唱苍白的故事 而你腼腆的笑容是故事的开始 迎面而来的微风像你说话的样子 没有任何预兆这故事戛然而止 人生总有一些些遗憾那就随它去 短暂的阳光也一样温暖了心灵 总有些时光值得怀念却又回不去 回不去的留不下的对你的回忆 人生总有一些些遗憾那就随它去 短暂的阳光也一样温暖了心灵 总有些时光值得怀念却又回不去 回不去的留不下的对你的回忆(Repeat till the end) 链接网易云]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[个人直播地址]]></title>
      <url>%2F2017%2F04%2F04%2Flive_url%2F</url>
      <content type="text"><![CDATA[经常游戏，日常音乐，偶尔电影。steam/origin：livexia网易云id：livexia bilibili:livexia]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[歌曲推荐：其实，我就在你方圆几里]]></title>
      <url>%2F2017%2F04%2F03%2Fmusic_1%2F</url>
      <content type="text"><![CDATA[演唱：Amy Chanrich 原唱：薛之谦 分类：翻唱 语种：华语 曲风：流行 其实，我就在你方圆几里Amy Chanrich 介绍： 搬运自5sing 其实，方圆几里，你还要我怎样，刚刚好四首串烧 不需要借口爱淡了就放手我不想听你也没说平静的交错 随便找个理由决定了就别回头不爱你的人说什么都没用 分开时难过不能说谁没谁不能好好过那天我们走了很久没有争吵过 分开时难过不要说如果被你一笑而过还不如让你选择想要的生活 与其在你不要的世界里不如痛快把你忘记这道理谁都懂 说容易 爱透了还要嘴硬我宁愿 留在你方圆几里我的心 要不回就送你因为我爱你 和你没关系 与其在你不要的世界里不如痛快把你忘记我都会轻描淡写仿佛没爱过其实我根本没人说其实我没你不能活可惜我 谁劝都不听 分开后我会笑着说当朋友问你关于我我都会轻描淡写仿佛没爱过其实我根本没人说陪你走的路你不能忘因为那是我 最快乐的时光 你别太在意我身上的记号 链接5Sing 网易云]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[The third week|第三周]]></title>
      <url>%2F2017%2F04%2F03%2Fpost_4%2F</url>
      <content type="text"><![CDATA[接上周： TODO：完善爬虫，完整解决输出格式问题，加入随机ua模块。 代码：livexia/douban_top250解决输出问题，整理输出格式，深入理解了items，pipeline。输出无错误。截图见github。 增加了随机UA模块，中间件为useragent.py,文件结构如下 ├── douban_top250 │ └── spiders │ ├── __init__.py │ ├── top250.py │ └── useragent.py │ …... 运行结果见github。 下周 TODO:加入其他反爬虫功能，尝试编写针对网易云的爬虫]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[The second week|第二周]]></title>
      <url>%2F2017%2F04%2F03%2Fpost_3%2F</url>
      <content type="text"><![CDATA[由于中文教程版本以及目标网站的下线，我改学英文最新的教程。 目标链接为：toscrape完成网站练习之后，我开始尝试爬取豆瓣电影top250，由于豆瓣使用了检测UA信息的反爬虫技术，但是我没有添加ua信息，导致爬虫返回403，没有取得任何结果。 在settings.py中加入ua，问题解决。 scrapy crawl top250 -o top250.json 启动spider，将结果储存在top250.json文件中，由于编码问题，json无法直接阅读。 scrapy crawl top250 -o top250.csv 启动spider，将结果储存在top250.csv文件中 利用Pipeline 处理爬取得到的数据，固定格式。 转化编码失败，通过pipeline错误。 TODO：完善爬虫，完整解决输出格式问题，加入随机ua模块。 代码：livexia/douban_top250]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[The first week|第一周]]></title>
      <url>%2F2017%2F04%2F03%2Fpost_2%2F</url>
      <content type="text"><![CDATA[入门学习scrapy系统：osx 按照scrapy-chs.readthedocs.io 目标网站 Open Directory Project 安装Scrapy Anaconda： conda install -c scrapinghub scrapy 创建项目tutorial $ scrapy startproject tutorial 项目结构 $ tree tutorial tutorial ├── scrapy.cfg └── tutorial ├── __init__.py ├── __pycache__ ├── items.py ├── middlewares.py ├── pipelines.py ├── settings.py └── spiders ├── __init__.py └── __pycache__ 定义item 用来存放名字、url、网站的描述 vim ./totorial/totorial/items.py import scrapy class DmozItem(scrapy.Item): title = scrapy.Field() link = scrapy.Field() desc = scrapy.Field() pass 写一个spider $ vim ./tutorial/tutorial/spiders/dmoz_spider.py import scrapy class DmozSpider(scrapy.Spider): name = &quot;dmoz&quot; allowed_domains = [&quot;dmoz.org&quot;] start_urls = [ &quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&quot;, &quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&quot; ] def parse(self, response): filename = response.url.split(&quot;/&quot;)[-2] + &apos;.html&apos; with open(filename, &apos;wb&apos;) as f: f.write(response.body) #name : 该spider的名字，唯一 #start_urls : 该spider的起始地址，后续的地址由起始地址爬取得到 #parse() 是spider的一个方法。 被调用时，每个初始URL完成下载后生成的 Response 对象将会作为唯一的参数传递给该函数。 该方法负责解析返回的数据(response data)，提取数据(生成item)以及生成需要进一步处理的URL的 Request 对象。 启动spider：根目录下 $ scrapy crawl dmoz 2017-03-16 14:55:22 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: tutorial) 2017-03-16 14:55:22 [scrapy.utils.log] INFO: Overridden settings: {&apos;BOT_NAME&apos;: &apos;tutorial&apos;, &apos;NEWSPIDER_MODULE&apos;: &apos;tutorial.spiders&apos;, &apos;ROBOTSTXT_OBEY&apos;: True, &apos;SPIDER_MODULES&apos;: [&apos;tutorial.spiders&apos;]} 2017-03-16 14:55:22 [scrapy.middleware] INFO: Enabled extensions: [&apos;scrapy.extensions.corestats.CoreStats&apos;, &apos;scrapy.extensions.telnet.TelnetConsole&apos;, &apos;scrapy.extensions.logstats.LogStats&apos;] 2017-03-16 14:55:23 [scrapy.middleware] INFO: Enabled downloader middlewares: [&apos;scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware&apos;, &apos;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&apos;, &apos;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware&apos;, &apos;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&apos;, &apos;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&apos;, &apos;scrapy.downloadermiddlewares.retry.RetryMiddleware&apos;, &apos;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&apos;, &apos;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&apos;, &apos;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&apos;, &apos;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&apos;, &apos;scrapy.downloadermiddlewares.stats.DownloaderStats&apos;] 2017-03-16 14:55:23 [scrapy.middleware] INFO: Enabled spider middlewares: [&apos;scrapy.spidermiddlewares.httperror.HttpErrorMiddleware&apos;, &apos;scrapy.spidermiddlewares.offsite.OffsiteMiddleware&apos;, &apos;scrapy.spidermiddlewares.referer.RefererMiddleware&apos;, &apos;scrapy.spidermiddlewares.urllength.UrlLengthMiddleware&apos;, &apos;scrapy.spidermiddlewares.depth.DepthMiddleware&apos;] 2017-03-16 14:55:23 [scrapy.middleware] INFO: Enabled item pipelines: [] 2017-03-16 14:55:23 [scrapy.core.engine] INFO: Spider opened 2017-03-16 14:55:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) 2017-03-16 14:55:23 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023 2017-03-16 14:55:25 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://www.dmoz.org/robots.txt&gt; (referer: None) 2017-03-16 14:55:26 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&gt; (referer: None) 2017-03-16 14:55:26 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt; (referer: None) 2017-03-16 14:55:26 [scrapy.core.engine] INFO: Closing spider (finished) 2017-03-16 14:55:26 [scrapy.statscollectors] INFO: Dumping Scrapy stats: {&apos;downloader/request_bytes&apos;: 734, &apos;downloader/request_count&apos;: 3, &apos;downloader/request_method_count/GET&apos;: 3, &apos;downloader/response_bytes&apos;: 16015, &apos;downloader/response_count&apos;: 3, &apos;downloader/response_status_count/200&apos;: 3, &apos;finish_reason&apos;: &apos;finished&apos;, &apos;finish_time&apos;: datetime.datetime(2017, 3, 16, 6, 55, 26, 472574), &apos;log_count/DEBUG&apos;: 4, &apos;log_count/INFO&apos;: 7, &apos;response_received_count&apos;: 3, &apos;scheduler/dequeued&apos;: 2, &apos;scheduler/dequeued/memory&apos;: 2, &apos;scheduler/enqueued&apos;: 2, &apos;scheduler/enqueued/memory&apos;: 2, &apos;start_time&apos;: datetime.datetime(2017, 3, 16, 6, 55, 23, 30734)} 2017-03-16 14:55:26 [scrapy.core.engine] INFO: Spider closed (finished) 目录结构 $ tree tutorial tutorial ├── Books.html ├── Resources.html ├── scrapy.cfg └── tutorial ├── __init__.py ├── __pycache__ │ ├── __init__.cpython-36.pyc │ └── settings.cpython-36.pyc ├── items.py ├── middlewares.py ├── pipelines.py ├── settings.py └── spiders ├── __init__.py ├── __pycache__ │ ├── __init__.cpython-36.pyc │ └── dmoz_spider.cpython-36.pyc └── dmoz_spider.py 提取数据XPath 周总结：周一、二学习了部分正则（没有做笔记），之后看了部分的urllib，在之后就是开始学习scrapy，目前才处于开始学习的阶段。因为期间电脑磁盘出了问题，学习时间有所下降。 下周目标：入门学习scrapy，完整实现爬取dmoz，尝试利用scrapy对豆瓣电影top 250进行爬取。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Innovation practice|创新实践 方向选择]]></title>
      <url>%2F2017%2F04%2F03%2Fpost_1%2F</url>
      <content type="text"><![CDATA[方向：利用Python抓取互联网信息学习目标 网页知识，浏览器抓包、拦截 异常处理，浏览器代码分析 多线程，多进程编程 爬虫模块：urllib,requests…. 正则表达式 爬虫框架Scrapy 代理池，模拟UA….. 主要目标 模拟登录网易云音乐，获取个人歌单，并抓取歌曲热门评论。 抓取游戏销售平台steam上所有游戏的价格及优惠信息。(酌情) 抓取当周销量销售前100游戏的热门评价(酌情)]]></content>
    </entry>

    
  
  
</search>
