<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[The fourth week|第四周]]></title>
      <url>%2F2017%2F04%2F09%2Fpost_5%2F</url>
      <content type="text"><![CDATA[TODO:加入其他反反爬虫功能，尝试编写针对网易云的爬虫 由于豆瓣top250没有什么反爬虫，于是我决定对网易云音乐下手，本周先对给定歌单进行获取，得到歌单的基本信息。 新建scrapy项目netease_music, 增加spider，name = getlist 尝试老方法对页面进行爬取书发现返回为js代码段。 由于网易云将所有实质内容存放在iframe标签呢，利用request方法无法取得页面内容。 在网上也看到有人利用selenium先得到iframe标签内内的html代码，再用xpath，对其进行获取。 我略微搜索发现selenium本身测试平台，拿来跑爬虫效率太低，便决定寻求其他方法。 之后在知乎上发现有网易云的api，github：metowolf/NeteaseCloudMusicApi，metowolf/Meting 采取使用网易云音乐的api方式来获取歌单信息。利用api可以简单获取到含有歌单所有信息的json文件，不需要scrapy进行爬取。 利用api也可以抓取评论，找到API简直省力几万倍 总结：本周大部分时间都花在如何解析iframe中，先有抓取方案过于臃肿浪费，找到网易云音乐api省事不少，具体实现本周尚未写好，由于api直接可以导出json，因此这周大部分针对抓取单一歌单的信息的努力直接实现。对于抓取到的.json文件，也可以通过python解析，获取需要的数据。 TODO：解析通过api获得的json文件，留取需要的数据，存入数据库中。模拟登陆网易云，爬取个人的所有歌单，利用网易云音乐api，获得json文件，将其存储在mongdb中。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[个人随想]]></title>
      <url>%2F2017%2F04%2F08%2Fcaprice%2F</url>
      <content type="text"><![CDATA[2017-04-08 23:58:38+08:00突然看到小说里面，陆总是被打成6，之前以为是巧用，今晚却突然想到有可能是，由于口音问题陆6不分。。。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[推荐过的歌曲]]></title>
      <url>%2F2017%2F04%2F05%2Fmusic_list%2F</url>
      <content type="text"><![CDATA[最近更新：2017-04-09 23:22:15+08:00 注：部分歌曲由于版权问题，无法在线播放 链接网易云]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[歌曲推荐：我的一个道姑朋友（Cover：Lon）]]></title>
      <url>%2F2017%2F04%2F05%2Fmusic_3%2F</url>
      <content type="text"><![CDATA[演唱：以冬 专辑：我的一个道姑朋友 语种：华语 曲风：流行、古风 我的一个道姑朋友（Cover：Lon）作曲 : 《一番星》作词 : 陆菱纱原唱：LON后期：圣雨轻纱 而你撑伞拥我入怀中，一字一句誓言多慎重。你眼中有柔情千种，如脉脉春风，冰雪也消融。那年长街春意正浓，策马同游，烟雨如梦。檐下躲雨，望进一双，深邃眼瞳，宛如华山夹着细雪的微风。 雨丝微凉，风吹过暗香朦胧。一时心头悸动，似你温柔剑锋，过处翩若惊鸿。 是否情字写来都空洞，一笔一画斟酌着奉送，甘愿卑微换个笑容，或沦为平庸。 而你撑伞拥我入怀中，一字一句誓言多慎重。你眼中有柔情千种，如脉脉春风，冰雪也消融。 后来谁家喜宴重逢，佳人在侧，烛影摇红。 灯火缱绻，映照一双，如画颜容，宛如豆蔻枝头温柔的旧梦。 对面不识，恍然间思绪翻涌。望你白衣如旧，神色几分冰冻，谁知我心惶恐？ 也许我应该趁醉装疯，借你怀抱留一抹唇红。再将旧事轻歌慢诵，任旁人惊动。 可我只能假笑扮从容，侧耳听那些情深意重。不去看你熟悉脸孔，只默默饮酒，多无动于衷。 山门外，雪拂过白衣，又在指尖消融；负长剑，试问江湖偌大，该何去何从？今生至此，像个笑话一样，自己都嘲讽，一厢情愿，有始无终。 若你早与他人两心同，何苦惹我错付了情衷。难道看我失魂落魄，你竟然心动？ 所幸经年漂浮红尘中，这颗心已是千疮百孔。怎惧你以薄情为刃，添一道裂缝？ 又不会痛。 不如将往事埋在风中，以长剑为碑，以霜雪为冢。此生若是错在相逢，求一个善终。 孤身打马南屏旧桥边过，恰逢山雨来时雾蒙蒙。想起那年伞下轻拥，就像躺在桥索之上，做了一场梦，梦醒后跌落，粉身碎骨，无影亦无踪。 链接网易云]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[歌曲推荐：遥远的歌]]></title>
      <url>%2F2017%2F04%2F04%2Fmusic_2%2F</url>
      <content type="text"><![CDATA[演唱：Cannie 专辑：Bittersweet 语种：华语 曲风：流行 遥远的歌词/曲/编曲/演唱：Cannie Y. 写了一首遥远的歌送给遥远的你 你的笑声我的歌声编织在一起 这是我对旧时光最温暖的回忆 哭着笑着痛着疯着跟过去别离 旧旧的琴弦在吟唱苍白的故事 而你腼腆的笑容是故事的开始 迎面而来的微风像你说话的样子 没有任何预兆这故事戛然而止 人生总有一些些遗憾那就随它去 短暂的阳光也一样温暖了心灵 总有些时光值得怀念却又回不去 回不去的留不下的对你的回忆 人生总有一些些遗憾那就随它去 短暂的阳光也一样温暖了心灵 总有些时光值得怀念却又回不去 回不去的留不下的对你的回忆(Repeat till the end) 链接网易云]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[个人直播地址]]></title>
      <url>%2F2017%2F04%2F04%2Flive_url%2F</url>
      <content type="text"><![CDATA[经常游戏，日常音乐，偶尔电影。steam/origin：livexia网易云id：livexia bilibili:livexia]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[歌曲推荐：其实，我就在你方圆几里]]></title>
      <url>%2F2017%2F04%2F03%2Fmusic_1%2F</url>
      <content type="text"><![CDATA[演唱：Amy Chanrich 原唱：薛之谦 分类：翻唱 语种：华语 曲风：流行 其实，我就在你方圆几里Amy Chanrich 介绍： 搬运自5sing 其实，方圆几里，你还要我怎样，刚刚好四首串烧 不需要借口爱淡了就放手我不想听你也没说平静的交错 随便找个理由决定了就别回头不爱你的人说什么都没用 分开时难过不能说谁没谁不能好好过那天我们走了很久没有争吵过 分开时难过不要说如果被你一笑而过还不如让你选择想要的生活 与其在你不要的世界里不如痛快把你忘记这道理谁都懂 说容易 爱透了还要嘴硬我宁愿 留在你方圆几里我的心 要不回就送你因为我爱你 和你没关系 与其在你不要的世界里不如痛快把你忘记我都会轻描淡写仿佛没爱过其实我根本没人说其实我没你不能活可惜我 谁劝都不听 分开后我会笑着说当朋友问你关于我我都会轻描淡写仿佛没爱过其实我根本没人说陪你走的路你不能忘因为那是我 最快乐的时光 你别太在意我身上的记号 链接5Sing 网易云]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[The third week|第三周]]></title>
      <url>%2F2017%2F04%2F03%2Fpost_4%2F</url>
      <content type="text"><![CDATA[接上周： TODO：完善爬虫，完整解决输出格式问题，加入随机ua模块。 代码：livexia/douban_top250解决输出问题，整理输出格式，深入理解了items，pipeline。输出无错误。截图见github。 增加了随机UA模块，中间件为useragent.py,文件结构如下 ├── douban_top250 │ └── spiders │ ├── __init__.py │ ├── top250.py │ └── useragent.py │ …... 运行结果见github。 下周 TODO:加入其他反爬虫功能，尝试编写针对网易云的爬虫]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[The second week|第二周]]></title>
      <url>%2F2017%2F04%2F03%2Fpost_3%2F</url>
      <content type="text"><![CDATA[由于中文教程版本以及目标网站的下线，我改学英文最新的教程。 目标链接为：toscrape完成网站练习之后，我开始尝试爬取豆瓣电影top250，由于豆瓣使用了检测UA信息的反爬虫技术，但是我没有添加ua信息，导致爬虫返回403，没有取得任何结果。 在settings.py中加入ua，问题解决。 scrapy crawl top250 -o top250.json 启动spider，将结果储存在top250.json文件中，由于编码问题，json无法直接阅读。 scrapy crawl top250 -o top250.csv 启动spider，将结果储存在top250.csv文件中 利用Pipeline 处理爬取得到的数据，固定格式。 转化编码失败，通过pipeline错误。 TODO：完善爬虫，完整解决输出格式问题，加入随机ua模块。 代码：livexia/douban_top250]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[The first week|第一周]]></title>
      <url>%2F2017%2F04%2F03%2Fpost_2%2F</url>
      <content type="text"><![CDATA[入门学习scrapy系统：osx 按照scrapy-chs.readthedocs.io 目标网站 Open Directory Project 安装Scrapy Anaconda： conda install -c scrapinghub scrapy 创建项目tutorial $ scrapy startproject tutorial 项目结构 $ tree tutorial tutorial ├── scrapy.cfg └── tutorial ├── __init__.py ├── __pycache__ ├── items.py ├── middlewares.py ├── pipelines.py ├── settings.py └── spiders ├── __init__.py └── __pycache__ 定义item 用来存放名字、url、网站的描述 vim ./totorial/totorial/items.py import scrapy class DmozItem(scrapy.Item): title = scrapy.Field() link = scrapy.Field() desc = scrapy.Field() pass 写一个spider $ vim ./tutorial/tutorial/spiders/dmoz_spider.py import scrapy class DmozSpider(scrapy.Spider): name = &quot;dmoz&quot; allowed_domains = [&quot;dmoz.org&quot;] start_urls = [ &quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&quot;, &quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&quot; ] def parse(self, response): filename = response.url.split(&quot;/&quot;)[-2] + &apos;.html&apos; with open(filename, &apos;wb&apos;) as f: f.write(response.body) #name : 该spider的名字，唯一 #start_urls : 该spider的起始地址，后续的地址由起始地址爬取得到 #parse() 是spider的一个方法。 被调用时，每个初始URL完成下载后生成的 Response 对象将会作为唯一的参数传递给该函数。 该方法负责解析返回的数据(response data)，提取数据(生成item)以及生成需要进一步处理的URL的 Request 对象。 启动spider：根目录下 $ scrapy crawl dmoz 2017-03-16 14:55:22 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: tutorial) 2017-03-16 14:55:22 [scrapy.utils.log] INFO: Overridden settings: {&apos;BOT_NAME&apos;: &apos;tutorial&apos;, &apos;NEWSPIDER_MODULE&apos;: &apos;tutorial.spiders&apos;, &apos;ROBOTSTXT_OBEY&apos;: True, &apos;SPIDER_MODULES&apos;: [&apos;tutorial.spiders&apos;]} 2017-03-16 14:55:22 [scrapy.middleware] INFO: Enabled extensions: [&apos;scrapy.extensions.corestats.CoreStats&apos;, &apos;scrapy.extensions.telnet.TelnetConsole&apos;, &apos;scrapy.extensions.logstats.LogStats&apos;] 2017-03-16 14:55:23 [scrapy.middleware] INFO: Enabled downloader middlewares: [&apos;scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware&apos;, &apos;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&apos;, &apos;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware&apos;, &apos;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&apos;, &apos;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&apos;, &apos;scrapy.downloadermiddlewares.retry.RetryMiddleware&apos;, &apos;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&apos;, &apos;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&apos;, &apos;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&apos;, &apos;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&apos;, &apos;scrapy.downloadermiddlewares.stats.DownloaderStats&apos;] 2017-03-16 14:55:23 [scrapy.middleware] INFO: Enabled spider middlewares: [&apos;scrapy.spidermiddlewares.httperror.HttpErrorMiddleware&apos;, &apos;scrapy.spidermiddlewares.offsite.OffsiteMiddleware&apos;, &apos;scrapy.spidermiddlewares.referer.RefererMiddleware&apos;, &apos;scrapy.spidermiddlewares.urllength.UrlLengthMiddleware&apos;, &apos;scrapy.spidermiddlewares.depth.DepthMiddleware&apos;] 2017-03-16 14:55:23 [scrapy.middleware] INFO: Enabled item pipelines: [] 2017-03-16 14:55:23 [scrapy.core.engine] INFO: Spider opened 2017-03-16 14:55:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) 2017-03-16 14:55:23 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023 2017-03-16 14:55:25 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://www.dmoz.org/robots.txt&gt; (referer: None) 2017-03-16 14:55:26 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&gt; (referer: None) 2017-03-16 14:55:26 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt; (referer: None) 2017-03-16 14:55:26 [scrapy.core.engine] INFO: Closing spider (finished) 2017-03-16 14:55:26 [scrapy.statscollectors] INFO: Dumping Scrapy stats: {&apos;downloader/request_bytes&apos;: 734, &apos;downloader/request_count&apos;: 3, &apos;downloader/request_method_count/GET&apos;: 3, &apos;downloader/response_bytes&apos;: 16015, &apos;downloader/response_count&apos;: 3, &apos;downloader/response_status_count/200&apos;: 3, &apos;finish_reason&apos;: &apos;finished&apos;, &apos;finish_time&apos;: datetime.datetime(2017, 3, 16, 6, 55, 26, 472574), &apos;log_count/DEBUG&apos;: 4, &apos;log_count/INFO&apos;: 7, &apos;response_received_count&apos;: 3, &apos;scheduler/dequeued&apos;: 2, &apos;scheduler/dequeued/memory&apos;: 2, &apos;scheduler/enqueued&apos;: 2, &apos;scheduler/enqueued/memory&apos;: 2, &apos;start_time&apos;: datetime.datetime(2017, 3, 16, 6, 55, 23, 30734)} 2017-03-16 14:55:26 [scrapy.core.engine] INFO: Spider closed (finished) 目录结构 $ tree tutorial tutorial ├── Books.html ├── Resources.html ├── scrapy.cfg └── tutorial ├── __init__.py ├── __pycache__ │ ├── __init__.cpython-36.pyc │ └── settings.cpython-36.pyc ├── items.py ├── middlewares.py ├── pipelines.py ├── settings.py └── spiders ├── __init__.py ├── __pycache__ │ ├── __init__.cpython-36.pyc │ └── dmoz_spider.cpython-36.pyc └── dmoz_spider.py 提取数据XPath 周总结：周一、二学习了部分正则（没有做笔记），之后看了部分的urllib，在之后就是开始学习scrapy，目前才处于开始学习的阶段。因为期间电脑磁盘出了问题，学习时间有所下降。 下周目标：入门学习scrapy，完整实现爬取dmoz，尝试利用scrapy对豆瓣电影top 250进行爬取。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Innovation practice|创新实践 方向选择]]></title>
      <url>%2F2017%2F04%2F03%2Fpost_1%2F</url>
      <content type="text"><![CDATA[方向：利用Python抓取互联网信息学习目标 网页知识，浏览器抓包、拦截 异常处理，浏览器代码分析 多线程，多进程编程 爬虫模块：urllib,requests…. 正则表达式 爬虫框架Scrapy 代理池，模拟UA….. 主要目标 模拟登录网易云音乐，获取个人歌单，并抓取歌曲热门评论。 抓取游戏销售平台steam上所有游戏的价格及优惠信息。(酌情) 抓取当周销量销售前100游戏的热门评价(酌情)]]></content>
    </entry>

    
  
  
</search>
