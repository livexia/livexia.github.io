<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>livexia</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://blog.livexia.site/"/>
  <updated>2017-04-05T08:21:12.000Z</updated>
  <id>http://blog.livexia.site/</id>
  
  <author>
    <name>livexia</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>歌曲推荐：遥远的歌</title>
    <link href="http://blog.livexia.site/2017/04/04/music_2/"/>
    <id>http://blog.livexia.site/2017/04/04/music_2/</id>
    <published>2017-04-04T14:17:27.000Z</published>
    <updated>2017-04-05T08:21:12.000Z</updated>
    
    <content type="html"><![CDATA[<hr>
<ul><br><li><em>演唱：</em><em>Cannie</em></li><br><li><em>专辑：</em><em>Bittersweet</em></li><br><li><em>语种：</em><em>华语</em></li><br><li><em>曲风：</em><em>流行&nbsp;&nbsp;</em></li><br></ul>

<hr>
<p><br></p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=418257688&auto=1&height=66"></iframe>

<p><br></p>
<p></p><h2>遥远的歌</h2><p></p>
<p></p><p>词/曲/编曲/演唱：Cannie Y.</p><br><br><br>写了一首遥远的歌送给遥远的你<br><br><br><br>你的笑声我的歌声编织在一起<br><br><br><br>这是我对旧时光最温暖的回忆<br><br><br><br>哭着笑着痛着疯着跟过去别离<br><br><br><br>旧旧的琴弦在吟唱苍白的故事<br><br><br><br>而你腼腆的笑容是故事的开始<br><br><br><br>迎面而来的微风像你说话的样子<br><br><br><br>没有任何预兆这故事戛然而止<br><br><br><br>人生总有一些些遗憾那就随它去<br><br><br><br>短暂的阳光也一样温暖了心灵<br><br><br><br>总有些时光值得怀念却又回不去<br><br><br><br>回不去的留不下的对你的回忆<br><br><br><br>人生总有一些些遗憾那就随它去<br><br><br><br>短暂的阳光也一样温暖了心灵<br><br><br><br>总有些时光值得怀念却又回不去<br><br><br><br>回不去的留不下的对你的回忆<br><br>(Repeat till the end)<p></p>
<hr>
<h2 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h2><p><a href="http://music.163.com/#/song/418257688" target="_blank" rel="external">网易云</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;ul&gt;&lt;br&gt;&lt;li&gt;&lt;em&gt;演唱：&lt;/em&gt;&lt;em&gt;Cannie&lt;/em&gt;&lt;/li&gt;&lt;br&gt;&lt;li&gt;&lt;em&gt;专辑：&lt;/em&gt;&lt;em&gt;Bittersweet&lt;/em&gt;&lt;/li&gt;&lt;br&gt;&lt;li&gt;&lt;em&gt;语种：&lt;/em&gt;&lt;em&gt;华语&lt;/em&gt;&lt;/li&gt;&lt;br&gt;&lt;li&gt;&lt;e
    
    </summary>
    
      <category term="Music" scheme="http://blog.livexia.site/categories/Music/"/>
    
    
      <category term="music" scheme="http://blog.livexia.site/tags/music/"/>
    
  </entry>
  
  <entry>
    <title>个人直播地址</title>
    <link href="http://blog.livexia.site/2017/04/04/live_url/"/>
    <id>http://blog.livexia.site/2017/04/04/live_url/</id>
    <published>2017-04-04T13:15:28.000Z</published>
    <updated>2017-04-05T08:21:21.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p>经常游戏，日常音乐，偶尔电影。</p>
<p>steam/origin：livexia</p>
<p>网易云id：livexia</p>
</blockquote>
<p><a href="https://live.bilibili.com/52802" target="_blank" rel="external">bilibili:livexia</a></p>
<embed height="540" width="96%" quality="high" allowfullscreen="true" type="application/x-shockwave-flash" src="https://static.hdslb.com/live-static/swf/LivePlayerEx_1.swf?room_id=52802&cid=52802&state=LIVE" flashvars="aid=7094514&page=1" pluginspage="//www.adobe.com/shockwave/download/download.cgi?P1_Prod_Version=ShockwaveFlash">

]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;经常游戏，日常音乐，偶尔电影。&lt;/p&gt;
&lt;p&gt;steam/origin：livexia&lt;/p&gt;
&lt;p&gt;网易云id：livexia&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://live.bilibili.com/528
    
    </summary>
    
      <category term="个人" scheme="http://blog.livexia.site/categories/%E4%B8%AA%E4%BA%BA/"/>
    
    
      <category term="直播" scheme="http://blog.livexia.site/tags/%E7%9B%B4%E6%92%AD/"/>
    
      <category term="视频" scheme="http://blog.livexia.site/tags/%E8%A7%86%E9%A2%91/"/>
    
  </entry>
  
  <entry>
    <title>歌曲推荐：其实，我就在你方圆几里</title>
    <link href="http://blog.livexia.site/2017/04/03/music_1/"/>
    <id>http://blog.livexia.site/2017/04/03/music_1/</id>
    <published>2017-04-03T09:15:08.000Z</published>
    <updated>2017-04-05T08:21:17.000Z</updated>
    
    <content type="html"><![CDATA[<hr>
<ul class="lt mb15"><br><li><em>演唱：</em><em>Amy Chanrich</em></li><br><li><em>原唱：</em><em>薛之谦</em></li><br><li><em>分类：</em><em>翻唱</em></li><br><li><em>语种：</em><em>华语</em></li><br><li><em>曲风：</em><em>流行&nbsp;&nbsp;</em></li><br></ul>

<hr>
<p><br></p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=437387277&auto=1&height=66"></iframe>

<p><br></p>
<p></p><h2>其实，我就在你方圆几里</h2><p></p>
<p>                 Amy Chanrich</p><br><br><br><p>介绍： 搬运自5sing<br><br><br><br>其实，方圆几里，你还要我怎样，刚刚好<br><br>四首串烧<br><br><br><br>不需要借口<br><br>爱淡了就放手<br><br>我不想听<br><br>你也没说平静的交错<br><br><br><br>随便找个理由<br><br>决定了就别回头<br><br>不爱你的人<br><br>说什么都没用<br><br><br><br>分开时难过不能说<br><br>谁没谁不能好好过<br><br>那天我们走了很久没有争吵过<br><br><br><br>分开时难过不要说<br><br>如果被你一笑而过<br><br>还不如让你选择想要的生活<br><br><br><br>与其在你不要的世界里<br><br>不如痛快把你忘记<br><br>这道理谁都懂 说容易 爱透了还要嘴硬<br><br>我宁愿 留在你方圆几里<br><br>我的心 要不回就送你<br><br>因为我爱你 和你没关系<br><br><br><br>与其在你不要的世界里<br><br>不如痛快把你忘记<br><br>我都会轻描淡写仿佛没爱过<br><br>其实我根本没人说<br><br>其实我没你不能活<br><br>可惜我 谁劝都不听<br><br><br><br>分开后我会笑着说<br><br>当朋友问你关于我<br><br>我都会轻描淡写仿佛没爱过<br><br>其实我根本没人说<br><br>陪你走的路你不能忘<br><br>因为那是我 最快乐的时光<br><br><br><br>你别太在意我身上的记号<br><br></p>

<hr>
<h2 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h2><p><a href="http://5sing.kugou.com/fc/15329562.html" target="_blank" rel="external">5Sing</a></p>
<p><a href="http://music.163.com/#/program?id=791256820" target="_blank" rel="external">网易云</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;ul class=&quot;lt mb15&quot;&gt;&lt;br&gt;&lt;li&gt;&lt;em&gt;演唱：&lt;/em&gt;&lt;em&gt;Amy Chanrich&lt;/em&gt;&lt;/li&gt;&lt;br&gt;&lt;li&gt;&lt;em&gt;原唱：&lt;/em&gt;&lt;em&gt;薛之谦&lt;/em&gt;&lt;/li&gt;&lt;br&gt;&lt;li&gt;&lt;em&gt;分类：&lt;/em&gt;&lt;em&gt;翻唱&lt;/em&gt;&lt;
    
    </summary>
    
      <category term="Music" scheme="http://blog.livexia.site/categories/Music/"/>
    
    
      <category term="music" scheme="http://blog.livexia.site/tags/music/"/>
    
  </entry>
  
  <entry>
    <title>The third week|第三周</title>
    <link href="http://blog.livexia.site/2017/04/03/post_4/"/>
    <id>http://blog.livexia.site/2017/04/03/post_4/</id>
    <published>2017-04-03T03:01:06.000Z</published>
    <updated>2017-04-05T08:20:53.000Z</updated>
    
    <content type="html"><![CDATA[<p> 接上周：</p>
<p>TODO：<del>完善爬虫，完整解决输出格式问题，加入随机ua模块。</del></p>
<p>代码：<a href="https://github.com/livexia/douban_top250" target="_blank" rel="external">livexia/douban_top250</a></p>
<p>解决输出问题，整理输出格式，深入理解了items，pipeline。输出无错误。截图见github。</p>
<p>增加了随机UA模块，中间件为useragent.py,文件结构如下</p>
<pre><code>├── douban_top250
│   └── spiders
│       ├── __init__.py
│       ├── top250.py
│       └── useragent.py
│    …...
</code></pre><p>运行结果见github。</p>
<p>下周</p>
<p>TODO:加入其他反爬虫功能，尝试编写针对网易云的爬虫</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt; 接上周：&lt;/p&gt;
&lt;p&gt;TODO：&lt;del&gt;完善爬虫，完整解决输出格式问题，加入随机ua模块。&lt;/del&gt;&lt;/p&gt;
&lt;p&gt;代码：&lt;a href=&quot;https://github.com/livexia/douban_top250&quot; target=&quot;_blank&quot; rel=&quot;
    
    </summary>
    
      <category term="创新实践" scheme="http://blog.livexia.site/categories/%E5%88%9B%E6%96%B0%E5%AE%9E%E8%B7%B5/"/>
    
    
      <category term="python" scheme="http://blog.livexia.site/tags/python/"/>
    
      <category term="scrapy" scheme="http://blog.livexia.site/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>The second week|第二周</title>
    <link href="http://blog.livexia.site/2017/04/03/post_3/"/>
    <id>http://blog.livexia.site/2017/04/03/post_3/</id>
    <published>2017-04-03T02:38:44.000Z</published>
    <updated>2017-04-05T08:20:57.000Z</updated>
    
    <content type="html"><![CDATA[<p> 由于中文教程版本以及目标网站的下线，我改学英文最新的教程。</p>
<p>目标链接为：<a href="http://quotes.toscrape.com/" target="_blank" rel="external">toscrape</a></p>
<p>完成网站练习之后，我开始尝试爬取豆瓣电影top250，由于豆瓣使用了检测UA信息的反爬虫技术，但是我没有添加ua信息，导致爬虫返回403，没有取得任何结果。</p>
<p>在settings.py中加入ua，问题解决。</p>
<pre><code>scrapy crawl top250 -o top250.json
</code></pre><p>启动spider，将结果储存在top250.json文件中，由于编码问题，json无法直接阅读。</p>
<pre><code>scrapy crawl top250 -o top250.csv
</code></pre><p>启动spider，将结果储存在top250.csv文件中</p>
<p>利用Pipeline 处理爬取得到的数据，固定格式。</p>
<p>转化编码失败，通过pipeline错误。</p>
<p>TODO：完善爬虫，完整解决输出格式问题，加入随机ua模块。</p>
<p>代码：<a href="https://github.com/livexia/douban_top250" target="_blank" rel="external">livexia/douban_top250</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt; 由于中文教程版本以及目标网站的下线，我改学英文最新的教程。&lt;/p&gt;
&lt;p&gt;目标链接为：&lt;a href=&quot;http://quotes.toscrape.com/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;toscrape&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;完成网站
    
    </summary>
    
      <category term="创新实践" scheme="http://blog.livexia.site/categories/%E5%88%9B%E6%96%B0%E5%AE%9E%E8%B7%B5/"/>
    
    
      <category term="python" scheme="http://blog.livexia.site/tags/python/"/>
    
      <category term="scrapy" scheme="http://blog.livexia.site/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>The first week|第一周</title>
    <link href="http://blog.livexia.site/2017/04/03/post_2/"/>
    <id>http://blog.livexia.site/2017/04/03/post_2/</id>
    <published>2017-04-03T02:14:58.000Z</published>
    <updated>2017-04-05T08:21:04.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="入门学习scrapy"><a href="#入门学习scrapy" class="headerlink" title="入门学习scrapy"></a>入门学习scrapy</h2><p>系统：osx</p>
<blockquote>
<p>按照<a href="http://scrapy-chs.readthedocs.io/zh_CN/1.0/intro/tutorial.html" target="_blank" rel="external">scrapy-chs.readthedocs.io</a></p>
<p>目标网站 <a href="http://www.dmoz.org/" target="_blank" rel="external">Open Directory Project</a></p>
</blockquote>
<p>安装Scrapy</p>
<p>Anaconda：</p>
<pre><code>conda install -c scrapinghub scrapy
</code></pre><p>创建项目tutorial</p>
<pre><code>$ scrapy startproject tutorial
</code></pre><p>项目结构</p>
<pre><code>$ tree tutorial
tutorial
├── scrapy.cfg
└── tutorial
   ├── __init__.py
   ├── __pycache__
   ├── items.py
   ├── middlewares.py
   ├── pipelines.py
   ├── settings.py
   └── spiders
       ├── __init__.py
       └── __pycache__
</code></pre><p>定义item</p>
<p>用来存放名字、url、网站的描述</p>
<pre><code>vim ./totorial/totorial/items.py
import scrapy

class DmozItem(scrapy.Item):
   title = scrapy.Field()
   link = scrapy.Field()
   desc = scrapy.Field()
   pass
</code></pre><p>写一个spider</p>
<pre><code>$ vim ./tutorial/tutorial/spiders/dmoz_spider.py

import scrapy

class DmozSpider(scrapy.Spider):
   name = &quot;dmoz&quot;
   allowed_domains = [&quot;dmoz.org&quot;]
   start_urls = [
       &quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&quot;,
       &quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&quot;
   ]

   def parse(self, response):
       filename = response.url.split(&quot;/&quot;)[-2] + &apos;.html&apos;
       with open(filename, &apos;wb&apos;) as f:
           f.write(response.body)

#name : 该spider的名字，唯一

#start_urls : 该spider的起始地址，后续的地址由起始地址爬取得到

#parse() 是spider的一个方法。 被调用时，每个初始URL完成下载后生成的 Response 对象将会作为唯一的参数传递给该函数。 该方法负责解析返回的数据(response data)，提取数据(生成item)以及生成需要进一步处理的URL的 Request 对象。
</code></pre><p>启动spider：<br>根目录下</p>
<pre><code>$ scrapy crawl dmoz
2017-03-16 14:55:22 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: tutorial)
2017-03-16 14:55:22 [scrapy.utils.log] INFO: Overridden settings: {&apos;BOT_NAME&apos;: &apos;tutorial&apos;, &apos;NEWSPIDER_MODULE&apos;: &apos;tutorial.spiders&apos;, &apos;ROBOTSTXT_OBEY&apos;: True, &apos;SPIDER_MODULES&apos;: [&apos;tutorial.spiders&apos;]}
2017-03-16 14:55:22 [scrapy.middleware] INFO: Enabled extensions:
[&apos;scrapy.extensions.corestats.CoreStats&apos;,
&apos;scrapy.extensions.telnet.TelnetConsole&apos;,
&apos;scrapy.extensions.logstats.LogStats&apos;]
2017-03-16 14:55:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
[&apos;scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware&apos;,
&apos;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&apos;,
&apos;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware&apos;,
&apos;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&apos;,
&apos;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&apos;,
&apos;scrapy.downloadermiddlewares.retry.RetryMiddleware&apos;,
&apos;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&apos;,
&apos;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&apos;,
&apos;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&apos;,
&apos;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&apos;,
&apos;scrapy.downloadermiddlewares.stats.DownloaderStats&apos;]
2017-03-16 14:55:23 [scrapy.middleware] INFO: Enabled spider middlewares:
[&apos;scrapy.spidermiddlewares.httperror.HttpErrorMiddleware&apos;,
&apos;scrapy.spidermiddlewares.offsite.OffsiteMiddleware&apos;,
&apos;scrapy.spidermiddlewares.referer.RefererMiddleware&apos;,
&apos;scrapy.spidermiddlewares.urllength.UrlLengthMiddleware&apos;,
&apos;scrapy.spidermiddlewares.depth.DepthMiddleware&apos;]
2017-03-16 14:55:23 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-03-16 14:55:23 [scrapy.core.engine] INFO: Spider opened
2017-03-16 14:55:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-16 14:55:23 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-03-16 14:55:25 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://www.dmoz.org/robots.txt&gt; (referer: None)
2017-03-16 14:55:26 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&gt; (referer: None)
2017-03-16 14:55:26 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt; (referer: None)
2017-03-16 14:55:26 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-16 14:55:26 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{&apos;downloader/request_bytes&apos;: 734,
&apos;downloader/request_count&apos;: 3,
&apos;downloader/request_method_count/GET&apos;: 3,
&apos;downloader/response_bytes&apos;: 16015,
&apos;downloader/response_count&apos;: 3,
&apos;downloader/response_status_count/200&apos;: 3,
&apos;finish_reason&apos;: &apos;finished&apos;,
&apos;finish_time&apos;: datetime.datetime(2017, 3, 16, 6, 55, 26, 472574),
&apos;log_count/DEBUG&apos;: 4,
&apos;log_count/INFO&apos;: 7,
&apos;response_received_count&apos;: 3,
&apos;scheduler/dequeued&apos;: 2,
&apos;scheduler/dequeued/memory&apos;: 2,
&apos;scheduler/enqueued&apos;: 2,
&apos;scheduler/enqueued/memory&apos;: 2,
&apos;start_time&apos;: datetime.datetime(2017, 3, 16, 6, 55, 23, 30734)}
2017-03-16 14:55:26 [scrapy.core.engine] INFO: Spider closed (finished)
</code></pre><p>目录结构</p>
<pre><code>$ tree tutorial
tutorial
├── Books.html
├── Resources.html
├── scrapy.cfg
└── tutorial
   ├── __init__.py
   ├── __pycache__
   │   ├── __init__.cpython-36.pyc
   │   └── settings.cpython-36.pyc
   ├── items.py
   ├── middlewares.py
   ├── pipelines.py
   ├── settings.py
   └── spiders
       ├── __init__.py
       ├── __pycache__
       │   ├── __init__.cpython-36.pyc
       │   └── dmoz_spider.cpython-36.pyc
       └── dmoz_spider.py
</code></pre><p>提取数据<br>XPath</p>
<p>周总结：周一、二学习了部分正则（没有做笔记），之后看了部分的urllib，在之后就是开始学习scrapy，目前才处于开始学习的阶段。<br>因为期间电脑磁盘出了问题，学习时间有所下降。</p>
<p>下周目标：入门学习scrapy，完整实现爬取dmoz，尝试利用scrapy对豆瓣电影top 250进行爬取。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;入门学习scrapy&quot;&gt;&lt;a href=&quot;#入门学习scrapy&quot; class=&quot;headerlink&quot; title=&quot;入门学习scrapy&quot;&gt;&lt;/a&gt;入门学习scrapy&lt;/h2&gt;&lt;p&gt;系统：osx&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;按照&lt;a href=&quot;
    
    </summary>
    
      <category term="创新实践" scheme="http://blog.livexia.site/categories/%E5%88%9B%E6%96%B0%E5%AE%9E%E8%B7%B5/"/>
    
    
      <category term="python" scheme="http://blog.livexia.site/tags/python/"/>
    
      <category term="scrapy" scheme="http://blog.livexia.site/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Innovation practice|创新实践 方向选择</title>
    <link href="http://blog.livexia.site/2017/04/03/post_1/"/>
    <id>http://blog.livexia.site/2017/04/03/post_1/</id>
    <published>2017-04-03T01:59:51.000Z</published>
    <updated>2017-04-05T08:21:08.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="方向：利用Python抓取互联网信息"><a href="#方向：利用Python抓取互联网信息" class="headerlink" title="方向：利用Python抓取互联网信息"></a><em>方向：利用Python抓取互联网信息</em></h1><h2 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h2><ul>
<li><p>网页知识，浏览器抓包、拦截</p>
</li>
<li><p>异常处理，浏览器代码分析</p>
</li>
<li><p>多线程，多进程编程</p>
</li>
<li><p>爬虫模块：urllib,requests….</p>
</li>
<li><p>正则表达式</p>
</li>
<li><p>爬虫框架Scrapy</p>
</li>
<li>代理池，模拟UA…..</li>
</ul>
<h2 id="主要目标"><a href="#主要目标" class="headerlink" title="主要目标"></a>主要目标</h2><ul>
<li><p>模拟登录网易云音乐，获取个人歌单，并抓取歌曲热门评论。</p>
</li>
<li><p>抓取游戏销售平台steam上所有游戏的价格及优惠信息。(酌情)</p>
</li>
<li><p>抓取当周销量销售前100游戏的热门评价(酌情)</p>
</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;方向：利用Python抓取互联网信息&quot;&gt;&lt;a href=&quot;#方向：利用Python抓取互联网信息&quot; class=&quot;headerlink&quot; title=&quot;方向：利用Python抓取互联网信息&quot;&gt;&lt;/a&gt;&lt;em&gt;方向：利用Python抓取互联网信息&lt;/em&gt;&lt;/h1&gt;
    
    </summary>
    
      <category term="创新实践" scheme="http://blog.livexia.site/categories/%E5%88%9B%E6%96%B0%E5%AE%9E%E8%B7%B5/"/>
    
    
      <category term="python" scheme="http://blog.livexia.site/tags/python/"/>
    
  </entry>
  
</feed>
